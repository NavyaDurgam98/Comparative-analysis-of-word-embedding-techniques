# -*- coding: utf-8 -*-
"""Twitter_Bert .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WJIXlZUu13TJi5AkdqksjQRma3RWvWui
"""

# Basic packages
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt

# Packages for data preparation
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

# Packages for modeling
from keras import models
from keras import layers
from transformers import BertTokenizer
import nltk
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
nltk.download('stopwords')

# Constants
NB_WORDS = 10000  # Parameter indicating the number of words in the dictionary
MAX_LEN = 24  # Maximum number of words in a sequence

def remove_stopwords(input_text):
    stopwords_list = stopwords.words('english')
    whitelist = ["n't", "not", "no"]
    words = input_text.split()
    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1]
    return " ".join(clean_words)

def remove_mentions(input_text):
    return re.sub(r'@\w+', '', input_text)

# Data cleaning
df = pd.read_csv('Tweets.csv')
df = df.reindex(np.random.permutation(df.index))
df = df[['text', 'airline_sentiment']]
df['text'] = df['text'].apply(remove_stopwords).apply(remove_mentions)

# Train and Test Split
X_train, X_test, y_train, y_test = train_test_split(df['text'], df['airline_sentiment'], test_size=0.1, random_state=37)

# Load pre-trained BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize text data using BERT tokenizer
X_train_bert = [tokenizer.encode(text, add_special_tokens=True, max_length=MAX_LEN, truncation=True) for text in X_train]
X_test_bert = [tokenizer.encode(text, add_special_tokens=True, max_length=MAX_LEN, truncation=True) for text in X_test]

# Pad sequences to ensure uniform length
X_train_bert_padded = pad_sequences(X_train_bert, maxlen=MAX_LEN, dtype="long", value=0, truncating="post", padding="post")
X_test_bert_padded = pad_sequences(X_test_bert, maxlen=MAX_LEN, dtype="long", value=0, truncating="post", padding="post")

# Converting target labels to numbers
le = LabelEncoder()
y_train_le = le.fit_transform(y_train)
y_test_le = le.transform(y_test)
y_train_oh = to_categorical(y_train_le)
y_test_oh = to_categorical(y_test_le)

# Model Architecture with BERT embeddings and Conv1D
model = models.Sequential()
model.add(layers.Input(shape=(MAX_LEN,), dtype="int32"))
model.add(layers.Embedding(len(tokenizer), 768, input_length=MAX_LEN, trainable=True))
model.add(layers.Conv1D(128, 5, activation='relu'))
model.add(layers.GlobalMaxPooling1D())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(3, activation='softmax'))

model.summary()

# Compile and train the model
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train_bert_padded, y_train_oh, epochs=20, batch_size=512, validation_split=0.1, verbose=1)

# Evaluate the model
y_pred_classes = np.argmax(model.predict(X_test_bert_padded), axis=1)
y_test_labels = le.transform(y_test)

# Calculate metrics
accuracy = accuracy_score(y_test_labels, y_pred_classes)
precision = precision_score(y_test_labels, y_pred_classes, average='macro')
recall = recall_score(y_test_labels, y_pred_classes, average='macro')
f1 = f1_score(y_test_labels, y_pred_classes, average='macro')

# Print metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)