# -*- coding: utf-8 -*-
"""Fasttext.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13la23veBtvuzw-K-7z-80yPsOqV4jcMu
"""

!pip install fasttext
# Basic packages
import pandas as pd
import numpy as np
import re
import collections
import matplotlib.pyplot as plt
from pathlib import Path

# Packages for data preparation
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

# Packages for modeling
from keras import models
from keras import layers
from keras import regularizers
import nltk
nltk.download('stopwords')

# Function to remove stopwords and mentions from text
def remove_stopwords_and_mentions(input_text):
    stopwords_list = stopwords.words('english')
    whitelist = ["n't", "not", "no"]
    words = input_text.split()
    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1]
    return " ".join(clean_words)

# Load data and preprocess
df = pd.read_csv('Tweets.csv')
df = df.reindex(np.random.permutation(df.index))
df = df[['text', 'airline_sentiment']]
df.text = df.text.apply(remove_stopwords_and_mentions)

# Train and test split
X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=37)
print('# Train data samples:', X_train.shape[0])
print('# Test data samples:', X_test.shape[0])

# Tokenization
tk = Tokenizer(num_words=NB_WORDS,
               filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
               lower=True,
               split=" ")
tk.fit_on_texts(X_train)

X_train_seq = tk.texts_to_sequences(X_train)
X_test_seq = tk.texts_to_sequences(X_test)

# Padding sequences
X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN)
X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN)

# Encode labels
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)
y_train_cat = to_categorical(y_train_enc)
y_test_cat = to_categorical(y_test_enc)

# Define embedding dimension
embedding_dim = 300

# Recalculate nb_words
nb_words = min(NB_WORDS, len(tk.word_index))

# Create embedding matrix
embedding_matrix = np.zeros((nb_words, embedding_dim))

# Load FastText word embeddings
import gensim.downloader as api
word_vectors = api.load("fasttext-wiki-news-subwords-300")

# Fill embedding matrix with FastText embeddings
for word, i in tk.word_index.items():
    if i >= NB_WORDS:
        continue
    try:
        embedding_vector = word_vectors[word]
        embedding_matrix[i] = embedding_vector
    except KeyError:
        embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), embedding_dim)

# Model architecture
model = models.Sequential()
model.add(layers.Embedding(NB_WORDS, embedding_dim, input_length=MAX_LEN, weights=[embedding_matrix], trainable=True))
model.add(layers.GlobalMaxPool1D())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.3))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.3))
model.add(layers.Dense(3, activation='softmax'))

# Compile and train the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train_pad, y_train_cat, epochs=NB_START_EPOCHS, batch_size=BATCH_SIZE, validation_split=0.1, verbose=1)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_pad, y_test_cat, verbose=0)
print('Test Accuracy: {:.2f}%'.format(accuracy * 100))

# Predictions
# Predictions
y_pred_probabilities = model.predict(X_test_pad)
y_pred = np.argmax(y_pred_probabilities, axis=1)


# Convert one-hot encoded labels back to categorical labels
y_test_cat_labels = np.argmax(y_test_cat, axis=1)

# Calculate precision, recall, and F1-score
from sklearn.metrics import precision_score, recall_score, f1_score
precision = precision_score(y_test_cat_labels, y_pred, average='weighted')
recall = recall_score(y_test_cat_labels, y_pred, average='weighted')
f1 = f1_score(y_test_cat_labels, y_pred, average='weighted')

print('Precision: {:.2f}'.format(precision))
print('Recall: {:.2f}'.format(recall))
print('F1 Score: {:.2f}'.format(f1))